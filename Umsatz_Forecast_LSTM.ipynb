{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9f0acd",
   "metadata": {},
   "source": [
    "# üéØ Retail Inventory Forecast - FINAL OPTIMAL\n",
    "\n",
    "**Projekt:** Vorhersage von Units Sold f√ºr Retail Stores  \n",
    "**Modell:** Bidirektionales LSTM mit 2 Layern  \n",
    "**Status:** Optimale Balance zwischen Varianz (Std ~10) und Overfitting (<1.3)\n",
    "\n",
    "**Optimierungen gegen√ºber Experiment 2.1:**\n",
    "- Batch Size: 256 ‚Üí 384 (glattere Gradienten)\n",
    "- Dropout: 0.2 ‚Üí 0.25 (weniger Overfitting)\n",
    "- L2 Reg: 0.0001 ‚Üí 0.00015 (st√§rkere Regularisierung)\n",
    "- Patience: 10 ‚Üí 8 (fr√ºher stoppen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8cdcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Zentrale Konfiguration - alle Parameter hier definiert!\"\"\"\n",
    "    # Pfade\n",
    "    data_path: str\n",
    "    target_col: str\n",
    "    \n",
    "    # Sequenz & Training\n",
    "    seq_length: int\n",
    "    test_size: float\n",
    "    batch_size: int\n",
    "    epochs: int\n",
    "    patience: int\n",
    "    \n",
    "    # Model Architektur\n",
    "    use_bidirectional: bool\n",
    "    lstm_layers: int\n",
    "    lstm_units_1: int\n",
    "    lstm_units_2: int\n",
    "    dense_units: int\n",
    "    dense_activation: str\n",
    "    dropout: float\n",
    "    l2_reg: float\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate: float\n",
    "    use_lr_scheduler: bool\n",
    "    lr_factor: float\n",
    "    lr_patience: int\n",
    "    lr_min: float\n",
    "    \n",
    "    # Feature Engineering\n",
    "    lag_periods: list\n",
    "    rolling_windows: list\n",
    "\n",
    "# üéØ FINAL OPTIMAL CONFIG - Balance zwischen Varianz und Overfitting\n",
    "config = Config(\n",
    "    # Pfade\n",
    "    data_path=\"/Users/mag/Library/Mobile Documents/com~apple~CloudDocs/Studium/7. Semester/Machine und Deep Learning/Bestands_Forecast/retail_store_inventory.csv\",\n",
    "    target_col='Units Sold',\n",
    "    \n",
    "    # Sequenz & Training\n",
    "    seq_length=60,              # Bew√§hrt: genug Kontext ohne zu viel Gl√§ttung\n",
    "    test_size=0.2,\n",
    "    batch_size=384,             # üî• Optimal zwischen 256 und 512\n",
    "    epochs=100,\n",
    "    patience=8,                 # üî• Fr√ºher stoppen: 10 ‚Üí 8\n",
    "    \n",
    "    # Model Architektur - GROSSE KAPAZIT√ÑT (wichtig f√ºr Varianz!)\n",
    "    use_bidirectional=True,     # Mehr Patterns erkennbar\n",
    "    lstm_layers=2,              # Hierarchisches Lernen\n",
    "    lstm_units_1=256,           # Gro√üe Kapazit√§t f√ºr Varianz (BEHALTEN!)\n",
    "    lstm_units_2=128,           # Bew√§hrt\n",
    "    dense_units=64,\n",
    "    dense_activation='relu',    # Standard f√ºr Dense Layers\n",
    "    dropout=0.25,               # üî• Leicht erh√∂ht: 0.2 ‚Üí 0.25\n",
    "    l2_reg=0.00015,             # üî• Leicht erh√∂ht: 0.0001 ‚Üí 0.00015\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=0.0002,       # Niedrig f√ºr stabiles Training\n",
    "    use_lr_scheduler=True,\n",
    "    lr_factor=0.7,\n",
    "    lr_patience=8,\n",
    "    lr_min=0.00001,\n",
    "    \n",
    "    # Feature Engineering\n",
    "    lag_periods=[1, 7, 30],     # 1 Tag, 1 Woche, 1 Monat\n",
    "    rolling_windows=[7, 30]     # Woche & Monat\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ FINAL OPTIMAL: Std 10+ & Overfitting <1.3\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Sequenz:          {config.seq_length} Tage\")\n",
    "print(f\"LSTM:             {config.lstm_layers} Layers, {config.lstm_units_1}‚Üí{config.lstm_units_2} Units ({'Bidirektional' if config.use_bidirectional else 'Unidirektional'})\")\n",
    "print(f\"Dense:            {config.dense_units} Units, {config.dense_activation}\")\n",
    "print(f\"Regularisierung:  Dropout={config.dropout} üî•, L2={config.l2_reg} üî•\")\n",
    "print(f\"Learning Rate:    {config.learning_rate}\")\n",
    "print(f\"Training:         {config.epochs} Epochen, Patience={config.patience} üî•\")\n",
    "print(f\"Batch Size:       {config.batch_size} üî•\")\n",
    "print()\n",
    "print(\"üî• = Optimiert gegen√ºber vorheriger Version\")\n",
    "print(\"Ziel: Overfitting 1.43 ‚Üí <1.3, Std ~10 beibehalten\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d2a84",
   "metadata": {},
   "source": [
    "## 1. Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e933c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"L√§dt und bereitet die Rohdaten vor.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    print(f\"‚úì Daten geladen: {df.shape}\")\n",
    "    print(f\"  Zeitraum: {df['Date'].min()} bis {df['Date'].max()}\")\n",
    "    print(f\"  Stores: {df['Store ID'].nunique()}, Products: {df['Product ID'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data(config.data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95196f04",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388513d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df: pd.DataFrame, config: Config) -> pd.DataFrame:\n",
    "    \"\"\"Erstellt zeitbasierte Features PRO Store-Product Gruppe.\"\"\"\n",
    "    \n",
    "    for (store, product), group in df.groupby(['Store_ID_Encoded', 'Product_ID_Encoded']):\n",
    "        idx = group.index\n",
    "        \n",
    "        # Lag Features\n",
    "        for lag in config.lag_periods:\n",
    "            df.loc[idx, f'{config.target_col}_lag_{lag}'] = group[config.target_col].shift(lag)\n",
    "        \n",
    "        # Rolling Features\n",
    "        for window in config.rolling_windows:\n",
    "            df.loc[idx, f'{config.target_col}_rolling_mean_{window}'] = group[config.target_col].rolling(window).mean()\n",
    "            df.loc[idx, f'{config.target_col}_rolling_std_{window}'] = group[config.target_col].rolling(window).std()\n",
    "        \n",
    "        # Diff Features\n",
    "        df.loc[idx, f'{config.target_col}_diff_1'] = group[config.target_col].diff(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df: pd.DataFrame, config: Config) -> pd.DataFrame:\n",
    "    \"\"\"Erstellt Features und encodiert kategoriale Variablen.\"\"\"\n",
    "    df['Store_ID_Encoded'] = df['Store ID'].astype('category').cat.codes\n",
    "    df['Product_ID_Encoded'] = df['Product ID'].astype('category').cat.codes\n",
    "    \n",
    "    df = df.sort_values(['Store_ID_Encoded', 'Product_ID_Encoded', 'Date']).reset_index(drop=True)\n",
    "    \n",
    "    df = create_temporal_features(df, config)\n",
    "    \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    df = pd.get_dummies(df, columns=['Category', 'Region', 'Weather Condition', 'Seasonality'])\n",
    "    df = df.drop(columns=['Store ID', 'Product ID'])\n",
    "    \n",
    "    print(f\"‚úì Features: {df.shape[1]} Spalten | Zeilen: {df.shape[0]}\")\n",
    "    return df\n",
    "\n",
    "df = engineer_features(df, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676d995",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dd5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df: pd.DataFrame, test_size: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Teilt Daten zeitbasiert in Train/Test.\"\"\"\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    df_train = df.iloc[:split_idx].copy()\n",
    "    df_test = df.iloc[split_idx:].copy()\n",
    "    \n",
    "    print(f\"‚úì Train: {len(df_train)}, Test: {len(df_test)}\")\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = train_test_split(df, config.test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1106a00",
   "metadata": {},
   "source": [
    "## 4. Skalierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2112a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df_train: pd.DataFrame, df_test: pd.DataFrame, target_col: str) -> Tuple[pd.DataFrame, pd.DataFrame, StandardScaler, StandardScaler, list]:\n",
    "    \"\"\"Skaliert Features und Target.\"\"\"\n",
    "    feature_cols = [col for col in df_train.columns \n",
    "                    if col not in [target_col, 'Date', 'Store_ID_Encoded', 'Product_ID_Encoded']]\n",
    "    \n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    df_train[feature_cols] = scaler_X.fit_transform(df_train[feature_cols])\n",
    "    df_test[feature_cols] = scaler_X.transform(df_test[feature_cols])\n",
    "    \n",
    "    df_train[[target_col]] = scaler_y.fit_transform(df_train[[target_col]])\n",
    "    df_test[[target_col]] = scaler_y.transform(df_test[[target_col]])\n",
    "    \n",
    "    print(f\"‚úì {len(feature_cols)} Features skaliert\")\n",
    "    return df_train, df_test, scaler_X, scaler_y, feature_cols\n",
    "\n",
    "df_train, df_test, scaler_X, scaler_y, feature_cols = scale_data(df_train, df_test, config.target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d3b1b",
   "metadata": {},
   "source": [
    "## 5. Sequenzen erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78223272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df: pd.DataFrame, feature_cols: list, target_col: str, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Erstellt Sequenzen PRO Store-Product Gruppe.\"\"\"\n",
    "    X_all, y_all = [], []\n",
    "    \n",
    "    for (store, product), group in df.groupby(['Store_ID_Encoded', 'Product_ID_Encoded']):\n",
    "        features = group[feature_cols].values\n",
    "        target = group[target_col].values\n",
    "        \n",
    "        for i in range(len(group) - seq_length):\n",
    "            X_all.append(features[i:i + seq_length])\n",
    "            y_all.append(target[i + seq_length])\n",
    "    \n",
    "    return np.array(X_all), np.array(y_all)\n",
    "\n",
    "X_train, y_train = create_sequences(df_train, feature_cols, config.target_col, config.seq_length)\n",
    "X_test, y_test = create_sequences(df_test, feature_cols, config.target_col, config.seq_length)\n",
    "\n",
    "print(f\"‚úì Sequenzen: Train {X_train.shape} | Test {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf465ca",
   "metadata": {},
   "source": [
    "## 6. LSTM Modell (Fast Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc9ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(config: Config, n_features: int) -> models.Sequential:\n",
    "    \"\"\"Erstellt LSTM-Modell basierend auf Config.\"\"\"\n",
    "    \n",
    "    l2_regularizer = tf.keras.regularizers.l2(config.l2_reg) if config.l2_reg > 0 else None\n",
    "    \n",
    "    model_layers = [layers.Input(shape=(config.seq_length, n_features))]\n",
    "    \n",
    "    # Erster LSTM Layer\n",
    "    if config.use_bidirectional:\n",
    "        model_layers.append(layers.Bidirectional(\n",
    "            layers.LSTM(\n",
    "                config.lstm_units_1, \n",
    "                return_sequences=(config.lstm_layers > 1),\n",
    "                kernel_regularizer=l2_regularizer,\n",
    "                recurrent_regularizer=l2_regularizer\n",
    "            )\n",
    "        ))\n",
    "    else:\n",
    "        model_layers.append(\n",
    "            layers.LSTM(\n",
    "                config.lstm_units_1, \n",
    "                return_sequences=(config.lstm_layers > 1),\n",
    "                kernel_regularizer=l2_regularizer,\n",
    "                recurrent_regularizer=l2_regularizer\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model_layers.append(layers.Dropout(config.dropout))\n",
    "    \n",
    "    # Zweiter LSTM Layer (optional)\n",
    "    if config.lstm_layers > 1:\n",
    "        if config.use_bidirectional:\n",
    "            model_layers.append(layers.Bidirectional(\n",
    "                layers.LSTM(config.lstm_units_2, return_sequences=False,\n",
    "                           kernel_regularizer=l2_regularizer, recurrent_regularizer=l2_regularizer)\n",
    "            ))\n",
    "        else:\n",
    "            model_layers.append(\n",
    "                layers.LSTM(config.lstm_units_2, return_sequences=False,\n",
    "                           kernel_regularizer=l2_regularizer, recurrent_regularizer=l2_regularizer)\n",
    "            )\n",
    "        \n",
    "        model_layers.append(layers.Dropout(config.dropout))\n",
    "    \n",
    "    # Dense Layers\n",
    "    model_layers.append(layers.Dense(config.dense_units, activation=config.dense_activation, \n",
    "                                     kernel_regularizer=l2_regularizer))\n",
    "    model_layers.append(layers.Dense(1))\n",
    "    \n",
    "    model = models.Sequential(model_layers)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_lstm_model(config, n_features=len(feature_cols))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40c76a",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c936f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: models.Sequential, X_train: np.ndarray, y_train: np.ndarray, \n",
    "                X_test: np.ndarray, y_test: np.ndarray, config: Config):\n",
    "    \"\"\"Trainiert das Modell mit Early Stopping und LR Scheduler.\"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=config.patience, restore_best_weights=True, monitor='val_loss', verbose=1)\n",
    "    ]\n",
    "    \n",
    "    if config.use_lr_scheduler:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=config.lr_factor, \n",
    "                patience=config.lr_patience, min_lr=config.lr_min, verbose=1\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    print(f\"üöÄ Training startet:\")\n",
    "    print(f\"   Epochen: {config.epochs} | Batch: {config.batch_size} | LR: {config.learning_rate}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "history = train_model(model, X_train, y_train, X_test, y_test, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìà TRAINING ABGESCHLOSSEN\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Beste Val Loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"Beste Val MAE: {min(history.history['val_mae']):.4f}\")\n",
    "\n",
    "train_loss = history.history['loss'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "ratio = val_loss / train_loss\n",
    "\n",
    "print(f\"\\nüîç Overfitting-Check: Ratio = {ratio:.2f}\")\n",
    "if ratio < 1.1:\n",
    "    print(\"  ‚úì Kein Overfitting\")\n",
    "elif ratio < 1.3:\n",
    "    print(\"  ‚ö†Ô∏è  Leichtes Overfitting\")\n",
    "else:\n",
    "    print(\"  ‚ùå Starkes Overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce4475",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862675bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: models.Sequential, X_test: np.ndarray, y_test: np.ndarray, \n",
    "                   scaler_y: StandardScaler) -> Tuple[np.ndarray, np.ndarray, dict]:\n",
    "    \"\"\"Evaluiert das Modell.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    y_pred_original = scaler_y.inverse_transform(y_pred).flatten()\n",
    "    \n",
    "    mae = np.mean(np.abs(y_test_original - y_pred_original))\n",
    "    rmse = np.sqrt(np.mean((y_test_original - y_pred_original)**2))\n",
    "    \n",
    "    metrics = {\n",
    "        'mae': mae, 'rmse': rmse,\n",
    "        'pred_mean': y_pred_original.mean(), 'pred_std': y_pred_original.std(),\n",
    "        'pred_min': y_pred_original.min(), 'pred_max': y_pred_original.max(),\n",
    "        'actual_mean': y_test_original.mean(), 'actual_std': y_test_original.std()\n",
    "    }\n",
    "    \n",
    "    return y_test_original, y_pred_original, metrics\n",
    "\n",
    "y_test_original, y_pred_original, metrics = evaluate_model(model, X_test, y_test, scaler_y)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"üìä ERGEBNISSE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MAE:  {metrics['mae']:.2f} | RMSE: {metrics['rmse']:.2f}\")\n",
    "print(f\"\\nPredictions: Mean={metrics['pred_mean']:.2f}, Std={metrics['pred_std']:.2f}\")\n",
    "print(f\"Actual:      Mean={metrics['actual_mean']:.2f}, Std={metrics['actual_std']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab4f26",
   "metadata": {},
   "source": [
    "## 9. Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history, y_test_original: np.ndarray, y_pred_original: np.ndarray):\n",
    "    \"\"\"Erstellt Visualisierungen.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], label='Train')\n",
    "    axes[0].plot(history.history['val_loss'], label='Val')\n",
    "    axes[0].set_title('Loss √ºber Epochen')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('MSE Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Scatter\n",
    "    idx = np.random.choice(len(y_test_original), min(500, len(y_test_original)), replace=False)\n",
    "    axes[1].scatter(y_test_original[idx], y_pred_original[idx], alpha=0.5, s=20)\n",
    "    axes[1].plot([50, 500], [50, 500], 'r--', lw=2)\n",
    "    axes[1].set_title('Predicted vs Actual')\n",
    "    axes[1].set_xlabel('Actual')\n",
    "    axes[1].set_ylabel('Predicted')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # Zeitreihe\n",
    "    n = min(200, len(y_test_original))\n",
    "    axes[2].plot(y_test_original[:n], label='Actual', alpha=0.7)\n",
    "    axes[2].plot(y_pred_original[:n], label='Predicted', alpha=0.7)\n",
    "    axes[2].set_title(f'Zeitreihe (erste {n} Samples)')\n",
    "    axes[2].set_xlabel('Sample')\n",
    "    axes[2].set_ylabel('Units Sold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(history, y_test_original, y_pred_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4adc2af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Modell-Architektur\n",
    "\n",
    "**LSTM Netzwerk:**\n",
    "- 2 Bidirektionale LSTM Layers (256‚Üí128 Units)\n",
    "- Dropout: 0.2 nach jedem Layer\n",
    "- Dense Layer: 64 Units mit ReLU\n",
    "- Output: 1 Unit (Regression)\n",
    "\n",
    "**Training:**\n",
    "- Optimizer: Adam (LR: 0.0002)\n",
    "- Loss: MSE mit MAE Metrik\n",
    "- Early Stopping: Patience 10\n",
    "- LR Scheduler: ReduceLROnPlateau\n",
    "\n",
    "**Daten:**\n",
    "- 60-Tage-Sequenzen\n",
    "- 100 Gruppen (5 Stores √ó 20 Products)\n",
    "- Features: Lag, Rolling, Diff + kategoriale Variablen\n",
    "- 80/20 Train/Test Split (zeitbasiert)\n",
    "\n",
    "**Ziel-Metriken:**\n",
    "- Prediction Std > 10 (Varianz)\n",
    "- MAE < 85\n",
    "- Overfitting Ratio < 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17509a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Ausf√ºhrung\n",
    "\n",
    "**Vollst√§ndiger Durchlauf:**\n",
    "1. `Run` ‚Üí `Run All Cells`\n",
    "2. Dauer: ~9-12 Minuten\n",
    "3. Ergebnisse in Zelle 18 (Training) und 20 (Evaluation)\n",
    "\n",
    "**Parameter anpassen:**\n",
    "1. √Ñndere Werte in Config-Zelle (Zelle 3)\n",
    "2. F√ºhre ab Zelle 13 neu aus (Sequenzen, Modell, Training)\n",
    "\n",
    "**Key-Metriken pr√ºfen:**\n",
    "- Overfitting Ratio (Zelle 18): sollte < 1.3 sein\n",
    "- Prediction Std (Zelle 20): sollte > 10 sein\n",
    "- MAE (Zelle 20): sollte < 90 sein\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
